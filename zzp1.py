import os, shutil
import numpy as np
os.chdir(os.path.dirname(os.path.abspath(__file__)))

import logging
mylog = logging.getLogger("mylog")
mylog.setLevel(logging.DEBUG)
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter("ğŸ•ï¸%(asctime)s [ğŸ¾%(levelname)sğŸ¾] %(pathname)s %(lineno)d %(funcName)sğŸˆï¸ %(message)sğŸ¦‰", datefmt="%y%m%d_%H%M%S"))
mylog.addHandler(handler)





# # nnï¼šèª¿æ•´ã™ã¹ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤é–¢æ•°ãŸã¡
# import torch.nn as nn
# # Fï¼šèª¿æ•´ã™ã¹ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒãŸãªã„é–¢æ•°ãŸã¡
# import torch.nn.functional as F

# # ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã®å›ºå®š
# torch.manual_seed(1)

# # æ™®é€šã¯ float32 ã§ä½œã‚‹ã€‚ãƒ‰ãƒƒãƒˆæ‰“ã¤ã¨æŒ‡å®šã§ãã‚‹ã€‚
# x = torch.tensor([[1., 2., 3.]])
# print(x.dtype)
# # torch.float32

# # ç·šå½¢å¤‰æ›ãŸã¡ã€‚åˆæœŸå€¤ã¯ãƒ©ãƒ³ãƒ€ãƒ ã€‚
# fc1 = nn.Linear(3, 2)
# fc2 = nn.Linear(2, 1)

# # é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã®ç¢ºèª
# print(fc1.weight)
# print(fc1.bias)
# # tensor([[ 0.2975, -0.2548, -0.1119],
# #         [ 0.2710, -0.5435,  0.3462]], requires_grad=True)
# # Parameter containing:
# # tensor([-0.1188,  0.2937], requires_grad=True)

# # ç·šå½¢å¤‰æ›ã®å®Ÿè¡Œ
# u1 = fc1(x)
# z1 = F.relu(u1)
# y = fc2(z1)

# print(y)
# # tensor([[0.1514]], grad_fn=<AddmmBackward0>)

# # ç›®æ¨™å€¤
# t = torch.tensor([[1.]])

# # å¹³å‡äºŒä¹—èª¤å·®
# loss = F.mse_loss(t, y)

# print(loss)
# # tensor(0.7201, grad_fn=<MseLossBackward0>)


# from sklearn.datasets import load_iris


# import torch
# import numpy as np

# from nn.network.dual_net import DualNet

# model_file_path = "model/sl-model_default.bin"
# model_file_path2 = "model/sl-model_20240912_011228_Ep:14.bin"

# device = torch.device("cpu")
# network = DualNet(device)
# network.to(torch.device("cpu"))
# try:
#     network.load_state_dict(torch.load(model_file_path))
# except Exception as e: # pylint: disable=W0702
#     print(f"ğŸ‘ºFailed to load {model_file_path}. : ", e)
#     raise e

# # ãŸã¶ã‚“ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è¡Œåˆ—ã®åå‰ã®ãƒªã‚¹ãƒˆãŒè¿”ã£ã¦ãã‚‹
# # print(network.state_dict().keys())

# # # ãŸã¶ã‚“ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è¡Œåˆ—ã®å€¤ãŒå…¨éƒ¨è¿”ã£ã¦ãã‚‹
# # print(network.state_dict())

# # # ãŸã¶ã‚“ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è¡Œåˆ—ã®å€¤ãŒå…¨éƒ¨è¿”ã£ã¦ãã‚‹
# # print(list(network.parameters()))

# print(network)



import glob
import os
import random
import numpy as np
from board.go_board import GoBoard
from board.stone import Stone
from nn.feature import generate_input_planes, generate_target_data, generate_rl_target_data
from sgf.reader import SGFReader
from learning_param import BATCH_SIZE, DATA_SET_SIZE
from typing import List


import time, datetime#################


def _save_data(save_file_path: str, input_data: np.ndarray, policy_data: np.ndarray, value_data: np.ndarray, kifu_counter: int) -> None:
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’npzãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›ã™ã‚‹ã€‚

    Args:
        save_file_path (str): ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€‚
        input_data (np.ndarray): å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã€‚
        policy_data (np.ndarray): Policyã®ãƒ‡ãƒ¼ã‚¿ã€‚
        value_data (np.ndarray): Valueã®ãƒ‡ãƒ¼ã‚¿
        kifu_counter (int): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ã‚‹æ£‹è­œãƒ‡ãƒ¼ã‚¿ã®å€‹æ•°ã€‚
    """
    save_data = {
        "input": np.array(input_data[0:DATA_SET_SIZE]),
        "policy": np.array(policy_data[0:DATA_SET_SIZE]),
        "value": np.array(value_data[0:DATA_SET_SIZE], dtype=np.int32),
        "kifu_count": np.array(kifu_counter)
    }
    np.savez_compressed(save_file_path, **save_data)

# pylint: disable=R0914
def generate_supervised_learning_data(program_dir: str, kifu_dir: str, board_size: int=9) -> None:
    """æ•™å¸«ã‚ã‚Šå­¦ç¿’ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¦ä¿å­˜ã™ã‚‹ã€‚

    Args:
        program_dir (str): ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ãƒ›ãƒ¼ãƒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
        kifu_dir (str): SGFãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ ¼ç´ã—ã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
        board_size (int, optional): ç¢ç›¤ã®ã‚µã‚¤ã‚º. Defaults to 9.
    """
    dt_watch = datetime.datetime.now()################
    print(f"ğŸ¾generate_supervised_learning_data {dt_watch}ğŸ¾")############

    board = GoBoard(board_size=board_size)

    input_data = []
    policy_data = []
    value_data = []

    kifu_counter = 1
    data_counter = 0

    kifu_num = len(glob.glob(os.path.join(kifu_dir, "*.sgf")))######
    print(f"kifu_num: {kifu_num}")#############

    # å±€ã®ãƒ«ãƒ¼ãƒ—
    for kifu_path in sorted(glob.glob(os.path.join(kifu_dir, "*.sgf"))):
        board.clear()
        # ã“ã“ã§å‹æ•—ã¨ã‹ã‚‚å–å¾—ã—ã¦ã‚‹
        sgf = SGFReader(kifu_path, board_size)
        color = Stone.BLACK
        value_label = sgf.get_value_label()
        """å‹ã¡è² ã‘"""

        # æ‰‹ã®ãƒ«ãƒ¼ãƒ—
        for pos in sgf.get_moves():
            # å¯¾ç§°å½¢ã§ã‹ã•å¢—ã—
            for sym in range(8):
                input_data.append(generate_input_planes(board, color, sym))
                policy_data.append(generate_target_data(board, pos, sym))
                value_data.append(value_label)
            board.put_stone(pos, color)
            color = Stone.get_opponent_color(color)
            # Valueã®ãƒ©ãƒ™ãƒ«ã‚’å…¥ã‚Œæ›¿ãˆã‚‹ã€‚
            value_label = 2 - value_label

        if len(value_data) >= DATA_SET_SIZE:
            _save_data(os.path.join(program_dir, "data", f"sl_data_{data_counter}"), input_data, policy_data, value_data, kifu_counter)
            input_data = input_data[DATA_SET_SIZE:]
            policy_data = policy_data[DATA_SET_SIZE:]
            value_data = value_data[DATA_SET_SIZE:]
            kifu_counter = 1
            data_counter += 1

            print(f"""
saved: sl_data_{data_counter}.npz ({datetime.datetime.now() - dt_watch})
from: {kifu_path} / {kifu_num}kyoku""")#####################
            dt_watch = datetime.datetime.now()

        kifu_counter += 1

    print("qwer")##########

    # ç«¯æ•°ã®å‡ºåŠ›
    n_batches = len(value_data) // BATCH_SIZE
    if n_batches > 0:
        _save_data(os.path.join(program_dir, "data", f"sl_data_{data_counter}"), input_data[0:n_batches*BATCH_SIZE], policy_data[0:n_batches*BATCH_SIZE], value_data[0:n_batches*BATCH_SIZE], kifu_counter)


npz = np.load("data/sl_data_0.npz")

print(npz["input"].shape)
print(npz["policy"].shape)
print(npz["value"].shape)