import os, shutil
import numpy as np
os.chdir(os.path.dirname(os.path.abspath(__file__)))

import logging
mylog = logging.getLogger("mylog")
mylog.setLevel(logging.DEBUG)
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter("ğŸ•ï¸%(asctime)s [ğŸ¾%(levelname)sğŸ¾] %(pathname)s %(lineno)d %(funcName)sğŸˆï¸ %(message)sğŸ¦‰", datefmt="%y%m%d_%H%M%S"))
mylog.addHandler(handler)





# # nnï¼šèª¿æ•´ã™ã¹ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤é–¢æ•°ãŸã¡
# import torch.nn as nn
# # Fï¼šèª¿æ•´ã™ã¹ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒãŸãªã„é–¢æ•°ãŸã¡
# import torch.nn.functional as F

# # ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã®å›ºå®š
# torch.manual_seed(1)

# # æ™®é€šã¯ float32 ã§ä½œã‚‹ã€‚ãƒ‰ãƒƒãƒˆæ‰“ã¤ã¨æŒ‡å®šã§ãã‚‹ã€‚
# x = torch.tensor([[1., 2., 3.]])
# print(x.dtype)
# # torch.float32

# # ç·šå½¢å¤‰æ›ãŸã¡ã€‚åˆæœŸå€¤ã¯ãƒ©ãƒ³ãƒ€ãƒ ã€‚
# fc1 = nn.Linear(3, 2)
# fc2 = nn.Linear(2, 1)

# # é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã®ç¢ºèª
# print(fc1.weight)
# print(fc1.bias)
# # tensor([[ 0.2975, -0.2548, -0.1119],
# #         [ 0.2710, -0.5435,  0.3462]], requires_grad=True)
# # Parameter containing:
# # tensor([-0.1188,  0.2937], requires_grad=True)

# # ç·šå½¢å¤‰æ›ã®å®Ÿè¡Œ
# u1 = fc1(x)
# z1 = F.relu(u1)
# y = fc2(z1)

# print(y)
# # tensor([[0.1514]], grad_fn=<AddmmBackward0>)

# # ç›®æ¨™å€¤
# t = torch.tensor([[1.]])

# # å¹³å‡äºŒä¹—èª¤å·®
# loss = F.mse_loss(t, y)

# print(loss)
# # tensor(0.7201, grad_fn=<MseLossBackward0>)


# from sklearn.datasets import load_iris


# import torch
# import numpy as np

# from nn.network.dual_net import DualNet

# model_file_path = "model/sl-model_default.bin"
# model_file_path2 = "model/sl-model_20240912_011228_Ep:14.bin"

# device = torch.device("cpu")
# network = DualNet(device)
# network.to(torch.device("cpu"))
# try:
#     network.load_state_dict(torch.load(model_file_path))
# except Exception as e: # pylint: disable=W0702
#     print(f"ğŸ‘ºFailed to load {model_file_path}. : ", e)
#     raise e

# # ãŸã¶ã‚“ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è¡Œåˆ—ã®åå‰ã®ãƒªã‚¹ãƒˆãŒè¿”ã£ã¦ãã‚‹
# # print(network.state_dict().keys())

# # # ãŸã¶ã‚“ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è¡Œåˆ—ã®å€¤ãŒå…¨éƒ¨è¿”ã£ã¦ãã‚‹
# # print(network.state_dict())

# # # ãŸã¶ã‚“ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è¡Œåˆ—ã®å€¤ãŒå…¨éƒ¨è¿”ã£ã¦ãã‚‹
# # print(list(network.parameters()))

# print(network)



import glob
import os
import random
import numpy as np
from board.go_board import GoBoard
from board.stone import Stone
from nn.feature import generate_input_planes, generate_target_data, generate_rl_target_data
from sgf.reader import SGFReader
from learning_param import BATCH_SIZE, DATA_SET_SIZE
from typing import List


import time, datetime#################


def _save_data(save_file_path: str, input_data: np.ndarray, policy_data: np.ndarray, value_data: np.ndarray, kifu_counter: int) -> None:
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’npzãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›ã™ã‚‹ã€‚

    Args:
        save_file_path (str): ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€‚
        input_data (np.ndarray): å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã€‚
        policy_data (np.ndarray): Policyã®ãƒ‡ãƒ¼ã‚¿ã€‚
        value_data (np.ndarray): Valueã®ãƒ‡ãƒ¼ã‚¿
        kifu_counter (int): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ã‚‹æ£‹è­œãƒ‡ãƒ¼ã‚¿ã®å€‹æ•°ã€‚
    """
    save_data = {
        "input": np.array(input_data[0:DATA_SET_SIZE]),
        "policy": np.array(policy_data[0:DATA_SET_SIZE]),
        "value": np.array(value_data[0:DATA_SET_SIZE], dtype=np.int32),
        "kifu_count": np.array(kifu_counter)
    }
    np.savez_compressed(save_file_path, **save_data)

# narr = np.random.randint(0, 10, (2, 3))
# print(narr)
# # [[9 7 5]
# #  [7 9 7]]

# narr = np.random.randint(0, 10, (2, 3, 4, 5))
# print(narr)
# # [[[0 2 8 5]
# #   [7 7 6 1]
# #   [3 4 1 0]]

# #  [[2 5 7 9]
# #   [5 9 7 4]
# #   [4 8 8 1]]]

# print(np.sum(narr))

# score = np.random.dirichlet(alpha=np.ones(10))
# print(score)

# a = np.arange(24)
# print(a)
# # [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]

# # 4 * 6 = 24 ã ã‹ã‚‰å¤‰æ›ã§ãã‚‹ã€‚
# print(a.reshape(4, 6))
# # [[ 0  1  2  3  4  5]
# #  [ 6  7  8  9 10 11]
# #  [12 13 14 15 16 17]
# #  [18 19 20 21 22 23]]

# # 2 * 3 * 4 = 24 ã ã‹ã‚‰å¤‰æ›ã§ãã‚‹ã€‚
# print(a.reshape(2, 3, 4))
# # [[[ 0  1  2  3]
# #   [ 4  5  6  7]
# #   [ 8  9 10 11]]

# #  [[12 13 14 15]
# #   [16 17 18 19]
# #   [20 21 22 23]]]

# # å¤‰æ›ã®é †ç•ªï¼ˆï¼Ÿï¼‰ã‚’æŒ‡å®šã§ãã‚‹ã€‚
# print(a.reshape(4, 6, order='F'))
# # [[ 0  4  8 12 16 20]
# #  [ 1  5  9 13 17 21]
# #  [ 2  6 10 14 18 22]
# #  [ 3  7 11 15 19 23]]

# # -1 ã‚’æŒ‡å®šã™ã‚‹ã¨ã€è‡ªå‹•ã§è¨ˆç®—ã—ã¦ãã‚Œã‚‹ã€‚
# print(a.reshape(-1, 6))
# # [[ 0  1  2  3  4  5]
# #  [ 6  7  8  9 10 11]
# #  [12 13 14 15 16 17]
# #  [18 19 20 21 22 23]]

# b = a.copy().reshape(2, 3, 4)
# """setumri"""

# b[1][1][1] = 100
# print(b)
# print(a)

# import sympy as sp


# # spã§å®£è¨€
# x = sp.symbols('x')

# # å¾®åˆ†
# print(sp.diff(x ** 2 * 3 + x, x))
# # 6*x + 1

# # é–¢æ•°ã§å¼ã‚’å®£è¨€ã—ã¦ã‚‚å¤§ä¸ˆå¤«
# def f (x):
#     return x ** 2 + 4 * x + 7
# print(sp.diff(f(x), x))
# # 2*x + 4

# # åå¾®åˆ†
# y = sp.symbols('y')
# print(sp.diff(x ** 3 * y + y, x))
# # 3*x**2*y

# # ç©åˆ†
# print(sp.integrate(x ** 2 * 3 + x, x))
# # 6*x + 1

# # sin ã¨ã‹æœ‰åå®šæ•°ã¯ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚Œã°ä½¿ãˆã‚‹
# from sympy import sin, pi

# # å®šç©åˆ†
# print(sp.integrate(sin(x), (x, 0, pi/2)))
# # 1

# # å¼ã¨ã—ã¦ç©åˆ†ã§ããªãã¦ã‚‚ã€evalf()ã§æ•°å€¤ç©åˆ†ã§ãã‚‹
# print(sp.integrate(sin(sin(x)), (x, 0, pi)).evalf())
# # 1.78648748195005

# # æ–¹ç¨‹å¼ x**2 - x - 6 = 0 ã‚’è§£ã
# print(sp.solve(x**2 - x - 6, x))
# # [-2, 3]

# # é€£ç«‹æ–¹ç¨‹å¼ã‚‚è§£ã‘ã‚‹
# print(sp.solve([x + y - 1, x - y - 1], [x, y]))
# # {x: 1, y: 0}

# # 1 ä»˜è¿‘ã‹ã‚‰ 8 æ¡ã®ç²¾åº¦ã§ã€è§£ã‚’æ¢ã™
# print(sp.nsolve(x**2 - x - 6, x, 1, prec=8))




# # nnï¼šèª¿æ•´ã™ã¹ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤é–¢æ•°ãŸã¡
# import torch.nn as nn
# # Fï¼šèª¿æ•´ã™ã¹ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒãŸãªã„é–¢æ•°ãŸã¡
# import torch.nn.functional as F

# # ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã®å›ºå®š
# torch.manual_seed(1)

# # æ™®é€šã¯ float32 ã§ä½œã‚‹ã€‚ãƒ‰ãƒƒãƒˆæ‰“ã¤ã¨æŒ‡å®šã§ãã‚‹ã€‚
# x = torch.tensor([[1., 2., 3.]])
# print(x.dtype)
# # torch.float32

# # ç·šå½¢å¤‰æ›ãŸã¡ã€‚åˆæœŸå€¤ã¯ãƒ©ãƒ³ãƒ€ãƒ ã€‚
# fc1 = nn.Linear(3, 2)
# fc2 = nn.Linear(2, 1)

# # é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã®ç¢ºèª
# print(fc1.weight)
# print(fc1.bias)
# # tensor([[ 0.2975, -0.2548, -0.1119],
# #         [ 0.2710, -0.5435,  0.3462]], requires_grad=True)
# # Parameter containing:
# # tensor([-0.1188,  0.2937], requires_grad=True)

# # ç·šå½¢å¤‰æ›ã®å®Ÿè¡Œ
# u1 = fc1(x)
# z1 = F.relu(u1)
# y = fc2(z1)

# print(y)
# # tensor([[0.1514]], grad_fn=<AddmmBackward0>)

# # ç›®æ¨™å€¤
# t = torch.tensor([[1.]])

# # å¹³å‡äºŒä¹—èª¤å·®
# loss = F.mse_loss(t, y)

# print(loss)
# # tensor(0.7201, grad_fn=<MseLossBackward0>)


# from sklearn.datasets import load_iris





import glob
import os
import random
import numpy as np
from board.go_board import GoBoard
from board.stone import Stone
from nn.feature import generate_input_planes, generate_target_data, generate_rl_target_data
from sgf.reader import SGFReader
from learning_param import BATCH_SIZE, DATA_SET_SIZE
from typing import List


import time, datetime#################


def _save_data(save_file_path: str, input_data: np.ndarray, policy_data: np.ndarray, value_data: np.ndarray, kifu_counter: int) -> None:
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’npzãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›ã™ã‚‹ã€‚

    Args:
        save_file_path (str): ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€‚
        input_data (np.ndarray): å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã€‚
        policy_data (np.ndarray): Policyã®ãƒ‡ãƒ¼ã‚¿ã€‚
        value_data (np.ndarray): Valueã®ãƒ‡ãƒ¼ã‚¿
        kifu_counter (int): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ã‚‹æ£‹è­œãƒ‡ãƒ¼ã‚¿ã®å€‹æ•°ã€‚
    """
    save_data = {
        "input": np.array(input_data[0:DATA_SET_SIZE]),
        "policy": np.array(policy_data[0:DATA_SET_SIZE]),
        "value": np.array(value_data[0:DATA_SET_SIZE], dtype=np.int32),
        "kifu_count": np.array(kifu_counter)
    }
    print("!!!!!!!!save")########
    np.savez_compressed(save_file_path, **save_data)

# pylint: disable=R0914
def generate_supervised_learning_data(program_dir: str, kifu_dir: str, board_size: int=9) -> None:
    """æ•™å¸«ã‚ã‚Šå­¦ç¿’ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¦ä¿å­˜ã™ã‚‹ã€‚

    Args:
        program_dir (str): ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ãƒ›ãƒ¼ãƒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
        kifu_dir (str): SGFãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ ¼ç´ã—ã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
        board_size (int, optional): ç¢ç›¤ã®ã‚µã‚¤ã‚º. Defaults to 9.
    """
    dt_watch = datetime.datetime.now()################
    print(f"ğŸ¾generate_supervised_learning_data {dt_watch}ğŸ¾")############

    board = GoBoard(board_size=board_size)

    input_data = []
    """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã€‚èª¬æ˜å¤‰æ•°ãŸã¡ã€‚"""
    policy_data = []
    """moveã®ãƒ‡ãƒ¼ã‚¿ã€‚ç›®çš„å¤‰æ•°ãŸã¡ã€‚"""
    value_data = []
    """å‹æ•—ã®ãƒ‡ãƒ¼ã‚¿ã€‚ç›®çš„å¤‰æ•°ï¼ŸãŸã¡ã€‚ï¼Ÿã“ã‚ŒãŒ 0 ã®ã¨ãå­¦ç¿’ã—ãªã„ï¼Ÿ"""

    kifu_counter = 1
    data_counter = 0
    """f"data/sl_data_{data_counter}"""

    kifu_num = len(glob.glob(os.path.join(kifu_dir, "*.sgf")))######
    print(f"kifu_num: {kifu_num}")#############

    ccc = 0#################
    # å±€ã®ãƒ«ãƒ¼ãƒ—
    for kifu_path in sorted(glob.glob(os.path.join(kifu_dir, "*.sgf"))):
        board.clear()
        # ã“ã“ã§å‹æ•—ã¨ã‹ã‚‚å–å¾—ã—ã¦ã‚‹
        sgf = SGFReader(kifu_path, board_size)
        color = Stone.BLACK
        value_label = sgf.get_value_label()
        """å‹ã¡è² ã‘ã€‚é»’å‹ã¡ã¯2ã€ç™½å‹ã¡ã¯0ã€æŒç¢ã¯1ã€‚"""
        cnt = 0#################
        # æ‰‹ã®ãƒ«ãƒ¼ãƒ—
        for pos in sgf.get_moves():
            cnt += 1##################
            # å¯¾ç§°å½¢ã§ã‹ã•å¢—ã—
            for sym in range(8):
                ccc += 1##################
                if sym == 0 and cnt < 10:###############
                    print(value_label)
                    print("-----------------")
                input_data.append(generate_input_planes(board, color, sym))
                policy_data.append(generate_target_data(board, pos, sym))
                value_data.append(value_label)
            board.put_stone(pos, color)
            color = Stone.get_opponent_color(color)
            # Valueã®ãƒ©ãƒ™ãƒ«ã‚’å…¥ã‚Œæ›¿ãˆã‚‹ã€‚
            # input_data ã®å±€é¢ã®æ‰‹ç•ªãŒå‹è€…ã®å ´åˆã¯ 2 ã«ã™ã‚‹ã€‚
            value_label = 2 - value_label
    
    
        print(f"kifu_counter: {kifu_counter}")#################
        print(f"data_counter: {data_counter}")#################
        print(f"cnt: {cnt}")#################

        if len(value_data) >= DATA_SET_SIZE:
            _save_data(os.path.join(program_dir, "backup", "test", f"sl_data_{data_counter}"), input_data, policy_data, value_data, kifu_counter)
            input_data = input_data[DATA_SET_SIZE:]
            policy_data = policy_data[DATA_SET_SIZE:]
            value_data = value_data[DATA_SET_SIZE:]
            kifu_counter = 1
            data_counter += 1

            print(f"""
saved: sl_data_{data_counter}.npz ({datetime.datetime.now() - dt_watch})
from: {kifu_path} / {kifu_num}kyoku""")#####################
            dt_watch = datetime.datetime.now()

        kifu_counter += 1

    print("qwer")##########
    print(f"ccc: {ccc}")#################

    # ç«¯æ•°ã®å‡ºåŠ›
    n_batches = len(value_data) // BATCH_SIZE
    if n_batches > 0:
        _save_data(os.path.join(program_dir, "backup", "test", f"sl_data_{data_counter}"), input_data[0:n_batches*BATCH_SIZE], policy_data[0:n_batches*BATCH_SIZE], value_data[0:n_batches*BATCH_SIZE], kifu_counter)

generate_supervised_learning_data(os.path.dirname(__file__), "archive/2")#################


# npz = np.load("backup/test/sl_data_0.npz")

# print(npz["input"].shape)
# print(npz["policy"].shape)
# print(npz["value"].shape)

