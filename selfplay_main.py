"""è‡ªå·±å¯¾æˆ¦ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã€‚
"""
import glob
import math
import os
import threading
import time
from concurrent.futures import ProcessPoolExecutor
import click
from board.constant import BOARD_SIZE
from selfplay.worker import selfplay_worker, selfplay_worker_vs,  display_selfplay_progress_worker
from learning_param import SELF_PLAY_VISITS, NUM_SELF_PLAY_WORKERS, \
    NUM_SELF_PLAY_GAMES
from monitoring import display_train_monitoring_worker#############
import datetime

import torch
import multiprocessing

# pylint: disable=R0913, R0914
@click.command()
@click.option('--save-dir', type=click.STRING, default="archive", \
    help="æ£‹è­œãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯archiveã€‚")
@click.option('--process', type=click.IntRange(min=1), default=NUM_SELF_PLAY_WORKERS, \
    help=f"è‡ªå·±å¯¾æˆ¦å®Ÿè¡Œãƒ¯ãƒ¼ã‚«æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯{NUM_SELF_PLAY_WORKERS}ã€‚")
@click.option('--num-data', type=click.IntRange(min=1), default=NUM_SELF_PLAY_GAMES, \
    help="ç”Ÿæˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿(æ£‹è­œ)ã®æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯10000ã€‚")
@click.option('--size', type=click.IntRange(2, BOARD_SIZE), default=BOARD_SIZE, \
    help=f"ç¢ç›¤ã®ã‚µã‚¤ã‚ºã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯{BOARD_SIZE}ã€‚")
@click.option('--use-gpu', type=click.BOOL, default=True, \
    help="GPUä½¿ç”¨ãƒ•ãƒ©ã‚°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Trueã€‚")
@click.option('--visits', type=click.IntRange(min=2), default=SELF_PLAY_VISITS, \
    help=f"è‡ªå·±å¯¾æˆ¦æ™‚ã®æ¢ç´¢å›æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯{SELF_PLAY_VISITS}ã€‚")
@click.option('--model', type=click.STRING, default=os.path.join("model_def", "sl-model_default.bin"), \
    help="ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯modelãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®sl-model_default.binã€‚")
@click.option('--model2', type=click.STRING, default="None", \
    help="ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’å¯¾å±€ã•ã›ã‚‹ã¨ãã«æŒ‡å®šã™ã‚‹ã€‚")
@click.option('--net', 'network_name1', type=click.STRING, default="DualNet", \
    help="--model ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ DualNetã€‚DualNet_256_24 ã¨ã‹ã‚’æŒ‡å®šã™ã‚‹ã€‚")
@click.option('--net2', 'network_name2', type=click.STRING, default="DualNet", \
    help="--model2 ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ DualNetã€‚")
def selfplay_main(save_dir: str, process: int, num_data: int, size: int, use_gpu: bool, visits: int, model: str, model2: str, network_name1: str, network_name2: str):
    """è‡ªå·±å¯¾æˆ¦ã‚’å®Ÿè¡Œã™ã‚‹ã€‚

    Args:
        save_dir (str): æ£‹è­œãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯archiveã€‚
        process (int): å®Ÿè¡Œã™ã‚‹è‡ªå·±å¯¾æˆ¦ãƒ—ãƒ­ã‚»ã‚¹æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯4ã€‚
        num_data (int): ç”Ÿæˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯10000ã€‚
        size (int): ç¢ç›¤ã®ã‚µã‚¤ã‚ºã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯BOARD_SIZEã€‚
        use_gpu (bool): GPUä½¿ç”¨ãƒ•ãƒ©ã‚°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯True
        visits (int): è‡ªå·±å¯¾æˆ¦å®Ÿè¡Œæ™‚ã®æ¢ç´¢å›æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯SELF_PLAY_VISITSã€‚
        model (str): ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯model/model.binã€‚
        model2 (str): ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Noneã€‚
        network_name1 (str): ä½¿ç”¨ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯DualNetã€‚
        network_name2 (str): ä½¿ç”¨ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯DualNetã€‚
    """

    import resource

    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’åˆ¶é™ï¼ˆå˜ä½ï¼šãƒã‚¤ãƒˆï¼‰#########
    soft, hard = resource.getrlimit(resource.RLIMIT_AS)
    resource.setrlimit(resource.RLIMIT_AS, (1024 * 1024 * 1024 * 8, hard))  # n GBã«åˆ¶é™
    print(f"ğŸ¾resource.getrlimit(resource.RLIMIT_AS): {resource.getrlimit(resource.RLIMIT_AS)}")###############

    monitoring_worker = threading.Thread(target=display_train_monitoring_worker, args=(use_gpu, True, 30, ), daemon=True)
    # monitoring_worker = threading.Thread(target=display_train_monitoring_worker, args=(use_gpu, True, 300, ), daemon=True)
    monitoring_worker.start()

    print("ğŸ¾model: ", model)#############
    print("ğŸ¾model2: ", model2)###############
    print("ğŸ¾network_name1: ", network_name1)###############
    print("ğŸ¾network_name2: ", network_name2)###############


    file_index_list = list(range(1, num_data + 1))
    split_size = math.ceil(num_data / process) # åˆ‡ã‚Šä¸Šã’
    file_indice = [file_index_list[i:i+split_size] for i in range(0, len(file_index_list), split_size)] # å¤šåˆ†ä¸¦è¡Œå‡¦ç†ã®ãŸã‚ã«åˆ†ã‘ã¦ã‚‹

    # num_data = 10, process = 4 ãªã‚‰
    # print(file_index_list)
    # # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    # print(file_indice)
    # # [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10]]

    # ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ã€æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª save_dir å†…ã®ãƒ•ã‚©ãƒ«ãƒ€åã‚’å–å¾—ã—ã€ãã®ä¸­ã§æœ€å¤§ã®æ•°å­—ã‚’è¦‹ã¤ã‘ã¦ã€æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ±ºå®šã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã™ã€‚
    kifu_dir_index_list = [int(os.path.split(dir_path)[-1]) for dir_path in glob.glob(os.path.join(save_dir, "*"))]
    kifu_dir_index_list.append(0)
    kifu_dir_index = max(kifu_dir_index_list) + 1

    start_time = time.time()
    os.mkdir(os.path.join(save_dir, str(kifu_dir_index)))

    print(f"Self play visits : {visits}")

    # GPUç•ªå·ã‚’å‰²ã‚Šå½“ã¦ã‚‹
    cnt = 0
    def gpu_num():
        if not torch.cuda.is_available():
            return -1
        nonlocal cnt
        cnt += 1
        print("cnt: ", cnt)###################
        print("cnt % torch.cuda.device_count(): ", cnt % torch.cuda.device_count())###################
        return cnt % torch.cuda.device_count()

    if model2 == "None":
        # ãƒ†ãƒ³ãƒ—ãƒ¬æ”¹é€ ï¼Ÿã“ã“ã§sgfã‚’å‡ºåŠ›ã—ã¦ãªã„selfplay_workerã§ã—ã¦ã‚‹ï¼Ÿ
        # submit(selfplay_worker,...ï¼ˆselfplay_workerã®å¼•æ•°ãŸã¡ï¼‰)ã‚‰ã—ã„
        # max_workers=process ã¯ä½¿ç”¨ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹æ•°ï¼Ÿ
        with ProcessPoolExecutor(max_workers=process) as executor:
            futures = [executor.submit(selfplay_worker, os.path.join(save_dir, str(kifu_dir_index)), model, file_list, size, visits, use_gpu, network_name1, gpu_num=gpu_num()) for file_list in file_indice]

            monitoring_worker = threading.Thread(target=display_selfplay_progress_worker, args=(os.path.join(save_dir, str(kifu_dir_index)), num_data, use_gpu), daemon=True);
            monitoring_worker.start()

            # ã“ã® .result() ã¯çµæœã‚’å‡ºåŠ›ã™ã‚‹ã®ãŒç›®çš„ã§ã¯ãªãã€æ­£å¸¸çµ‚äº†ã®ç¢ºèªãŒç›®çš„ã€‚
            # å¤šåˆ†ã€executor.shutdown(wait=True) ã§ã‚‚è‰¯ã„ã€‚
            for future in futures:
                future.result()
    else:
        with ProcessPoolExecutor(max_workers=process) as executor:
            futures = [executor.submit(selfplay_worker_vs, os.path.join(save_dir, str(kifu_dir_index)), model, model2, file_list, size, visits, use_gpu, network_name1, network_name2, gpu_num=gpu_num()) for file_list in file_indice]

            monitoring_worker = threading.Thread(target=display_selfplay_progress_worker, args=(os.path.join(save_dir, str(kifu_dir_index)), num_data, use_gpu), daemon=True);
            monitoring_worker.start()

            for future in futures:
                future.result()

    finish_time = time.time() - start_time

# f"[{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}] generating\n

    print(f"[{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}] generating_finish\n{finish_time:3f} seconds, {(3600.0 * num_data / finish_time):3f} games/hour")


if __name__ == "__main__":
    multiprocessing.set_start_method('spawn')
    selfplay_main() # pylint: disable=E1120